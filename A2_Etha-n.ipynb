{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOizbMu20leU"
   },
   "source": [
    "# edX Natural Language Processing Foundations – Assignment 2: Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone, this assignment notebook covers the **N-gram Language Models**. There are some code-completion tasks in this notebook. For code completion tasks, please write down your answer (i.e., your lines of code) between the sentences that \"Your code starts here\" and \"Your code ends here\". *The space between these two lines does not reflect the required or expected lines of code.*\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:**\n",
    "* Remember to rename and save this Jupyter notebook as **A2_edXusername.ipynb** (e.g., **A2_bobsmith.ipynb**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "* Remember to rename and save the script file **A2_script.py** as **A2_edXusername.py** (e.g., **A2_bobsmith.py**) before submission! Failure to do so will yield a penalty of 1 Point.\n",
    "\n",
    "Please also add your name and edX id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "edx_username = 'Etha-n'  # e.g., bobsmith, you can check this in edX `Account Settings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42aH_rjr57wh"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In our daily lives, we frequently search the web to gain new knowledge and type messages on the phone to share information. When you're in it, do you feel that something interesting is happening? You may notice that both the search engine and phone keyboard are automatically suggesting some words!\n",
    "\n",
    "This is exactly what we focus on in this assignment: **Language Models!** Briefly speaking, models that assign probabilities to sequences of words are called language models.\n",
    "\n",
    "Actually, we see language models in action every day. Besides the above two cases, imagine that you are writing some texts, some intelligent tools, such as *spelling correction* or *grammatical error correction*, are also language models, which provide you with some revision advice.\n",
    "\n",
    "But how does this work?\n",
    "\n",
    "Let's start with this assignment, which will help you implement your first Language Model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Python Environment\n",
    "We need to configure the Python environment before we start this assignment.\n",
    "Thus, we provide tutorials for setting up the environment in the directory `Python_Env_Setup`, you can follow the guidelines to finish the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making all the required imports. In the `util.py`, we already provided you with some useful functions to let you focus on the core implementation of the Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A2_script.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from A2_Etha_n import MyNgramLM # Would not let me import from A2_Etha-n\n",
    "# from A2_bobsmith import MyNgramLM # <-- you will need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Text Corpus\n",
    "\n",
    "To train a Language Model, we need a text corpus. Ideally, this corpus should be very large and representative to cover a wide range of n-grams. In this assignment, we use publicly available books that can be downloaded from [Project Gutenberg](https://www.gutenberg.org/). Most to all books are available as plain text file for use to use to train a model.\n",
    "\n",
    "### Download Pride and Prejudice from Project Gutenberg\n",
    "\n",
    "In this assignment, we use [Pride and Prejudice](https://www.gutenberg.org/files/1342/1342-0.txt) as our data resource.\n",
    "The following code cell downloads a book's plain text file to store it locally in the folder `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: directly run this code block.\n",
    "util.prepare_corpus(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "After downloading the corpus, you will find a `txt` file named  `pride_and_prejudice.txt` located in the path `./data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 11886 lines in \"pride_and_prejudice.txt\":\n",
      "Examples:\n",
      "superiority in point of construction, which, as it seems to me, it\n",
      "possesses over all the others. The plot, though not elaborate, is almost\n",
      "regular enough for Fielding; hardly a character, hardly an incident\n"
     ]
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "texts = util.get_corpus(\"./data/pride_and_prejudice.txt\")\n",
    "print(\"Examples:\")\n",
    "print(\"\\n\".join(texts[100:103])) # you can change the number to view different examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset\n",
    "Before we train our N-gram language model, it is necessary to make sure the data we put in it is in the right format.\n",
    "First of all, we need to tokenize the above `texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11886/11886 [02:23<00:00, 82.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "[['the', 'project', 'gutenberg', 'ebook', 'of', 'pride', 'and', 'prejudice', ',', 'by', 'jane', 'austen'], ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and'], ['most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "tokenized_texts = util.tokenize_texts(texts, lowercase=True)  # besides tokenization, we also lowercase all texts.\n",
    "print(\"Examples:\")\n",
    "print(tokenized_texts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Language Model\n",
    "Given the tokenized corpus, you’ll need to develop a class called `MyNgramLM`.\n",
    "\n",
    "The class needs to handle the following two arguments:\n",
    "\n",
    "* `n` (integer; default `2`) specifies the number of grams for constructing the language model. *e.g.,* `2` would correspond to the bigram model. In this assignment, we focus on the **Bigram Language Model**.\n",
    "* `k` (floating point number; default `1.0`) specifies the amount of smoothing to apply to the model. Take note to add this float to the counts of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize a 2-gram model with add-1-smoothing.\n"
     ]
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "ngram_lm = MyNgramLM(n=2, k=1) # in this assignment, we focus on bi-gram language model and employ add-1 smoothing method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding text\n",
    "Now, let's deal with some practical issues. Since we are working on the bigram language model, we need one word as the previous word. So, think about the first word of a given sentence, you will notice that it does not have a previous word! Besides, for the last word of the given sentence, it can serve as the previous word, but there is not a following word.\n",
    "\n",
    "A standard way to deal with this issue is to add special “padding” symbols to the sentence before splitting it into ngrams.\n",
    "\n",
    "Accordingly, **you will need one function to pad sentences to solve the above issue:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:  [['<s>', 'the', 'project', 'gutenberg', 'ebook', 'of', 'pride', 'and', 'prejudice', ',', 'by', 'jane', 'austen', '</s>'], ['<s>', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', '</s>'], ['<s>', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `add_padding` in the `py` file, then you can run this code block\n",
    "# Note: account for 10 points\n",
    "padding_texts = ngram_lm.add_padding(tokenized_texts)\n",
    "print(\"Examples: \", padding_texts[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are “known” to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7350\n"
     ]
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `build_vocabulary` in the `py` file, then you can run this code block\n",
    "# Note: account for 10 points\n",
    "ngram_lm.build_vocabulary(padding_texts, cutoff_freq=0) # you can change the value of `cutoff_freq` by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the vocabulary, we offer you a function `vocab_lookup`. Thus, you can easily transform inputs by replacing unknown words (words not included in the vocabulary) into `[UNK]` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am [UNK] .\n",
      "its first time .\n",
      "['i', 'am', '[UNK]', '.']\n",
      "['its', 'first', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "# for string input\n",
    "print(ngram_lm.vocab_lookup(\"i am bob .\"))\n",
    "print(ngram_lm.vocab_lookup(\"its first time .\"))\n",
    "# for list input\n",
    "print(ngram_lm.vocab_lookup([\"i\", \"am\", \"bob\", \".\"]))\n",
    "print(ngram_lm.vocab_lookup([\"its\", \"first\", \"time\", \".\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get N-grams\n",
    "Do you still remember what is a bigram language model? Recall that based on the Markov assumption, we approximate the history by using just one word: $P\\left(w_{1: n}\\right) \\approx \\prod_{k=1}^n P\\left(w_k | w_{k-1}\\right)$, where $w_{1: n}$ is one sentence with $n$ words.\n",
    "\n",
    "Now comes to the question: How to compute $P\\left(w_k | w_{k-1}\\right)$? The answer is: $P\\left(w_n | w_{n-1}\\right)=\\frac{C\\left(w_{n-1} w_n\\right)}{C\\left(w_{n-1}\\right)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: [('almost', 'no'), ('no', 'restrictions'), ('restrictions', '</s>'), ('<s>', 'whatsoever'), ('whatsoever', '.'), ('.', 'you'), ('you', 'may'), ('may', 'copy'), ('copy', 'it'), ('it', ',')]\n"
     ]
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `get_ngrams` in the `py` file, then you can run this code block\n",
    "# Note: account for 20 points\n",
    "ngrams = ngram_lm.get_ngrams(padding_texts)\n",
    "print(\"Examples:\", ngrams[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Having prepared our data we are ready to start training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Language Model Training.\n",
      "Number of n-grams: 56625\n"
     ]
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `fit` in the `py` file, then you can run this code block\n",
    "# Note: account for 20 points\n",
    "ngram_lm.fit(ngrams)\n",
    "print(\"Number of n-grams: {}\".format(len(ngram_lm.ngram_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating\n",
    "\n",
    "After the training, the model is ready to generate new sentences utilizing the probabilities of the n-grams based on the now available n-gram counts. It is worth noting that the key to generating texts from a language model is *sampling the next word's probability*.\n",
    "\n",
    "In this assignment, the start/seed words represented by `start_context` (more specifically, the last `(self.n - 1)` in `start_context`) are an existing n-gram.\n",
    "\n",
    "Is this enough? Can we start the generation process? Not really!\n",
    "\n",
    "We need one important method, namely *smoothing*, to make our bigram language model feasible. Basically, smoothing methods shave off a bit of probability mass from some more frequent word sequences and give it to word sequences we’ve never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add-K-smoothing\n",
    "\n",
    "In this assignment, **we focus on *add-k smoothing***.\n",
    "\n",
    "$P_{\\text {Add-k }}\\left(w_n | w_{n-1}\\right)=\\frac{C\\left(w_{n-1} w_n\\right)+k}{C\\left(w_{n-1}\\right)+k V}$\n",
    "\n",
    "where $V$ is the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03956079973779089"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `calc_prob` in the `py` file, then you can run this code block\n",
    "# Note: account for 20 points\n",
    "ngram_lm.calc_prob(context=\"i\", token=\"am\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given context and a candidate word, we can obtain the conditional probability via `calc_prob`.\n",
    "Therefore, given one context, you can enumerate all candidates to \"choose\" the word based on probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'had'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: you need to finish the code completion task for `random_token` in the `py` file, then you can run this code block\n",
    "# Note: account for 20 points\n",
    "ngram_lm.random_token(context=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, its time to start generation!\n",
    "\n",
    "*Since we pick the next words (kind of) randomly, executing the same example multiple times will generally yield different sentences. Try different start/seed words and see the generated sentences using multiple runs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they had now entered a beautiful walk by her , that'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "# Run it multiple times and you may get different results.\n",
    "# The generation process terminates when eos is generated or when the specified length is reached.\n",
    "ngram_lm.generate_text(token_count=20, start_context=\"they had now entered a beautiful walk by\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pride and'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_lm.generate_text(token_count=20, start_context=\"pride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the snakes entered a beautiful walk by the buildings .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "# Run it multiple times and you may get different results.\n",
    "# The generation process terminates when eos is generated or when the specified length is reached.\n",
    "ngram_lm.generate_text(token_count=20, start_context=\"the snakes entered a beautiful walk by the buildings . \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they had now entered a walk by the'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: directly run this code block.\n",
    "# Run it multiple times and you may get different results.\n",
    "# The generation process terminates when eos is generated or when the specified length is reached.\n",
    "ngram_lm.generate_text(token_count=20, start_context=\"they had now entered a walk by\")\n",
    "\n",
    "# The only reason it takes the then an end of line is because this LM thinks that the end of a line (not a sentence)\n",
    "# signifies </s>. Otherwise, I don't think it would have </s> after the as often, if at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this assignment we put Language Models to use for text generation. The underlying idea is that the conditional probabilities of the Language model allows us to predict the next word given a current context of words.\n",
    "Congratulations on finishing this assignment!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
